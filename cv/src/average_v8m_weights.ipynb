{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49cfc006-f81c-449b-a83d-851edec3c590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "816b8e1a-d5a1-4825-9ad4-f60f6786997a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the models\n",
    "model1 = YOLO('yolov8m_1920_finetune.pt') # v1\n",
    "model2 = YOLO('v8m_finetune.pt') # v2\n",
    "model3 = YOLO('v8m_finetune_stages.pt') # v3\n",
    "# Get state dictionaries\n",
    "state_dict1 = model1.model.state_dict()\n",
    "state_dict2 = model2.model.state_dict()\n",
    "state_dict3 = model3.model.state_dict()\n",
    "\n",
    "# Initialize an empty dictionary for the averaged state\n",
    "averaged_state_dict = {}\n",
    "\n",
    "# Average the weights\n",
    "for key in state_dict1.keys():\n",
    "    if key in state_dict2 and key in state_dict3:\n",
    "        averaged_state_dict[key] = (state_dict2[key] + state_dict3[key]) / 2\n",
    "        # print(averaged_state_dict[key])\n",
    "    else:\n",
    "        print(\"ERROR: KEY DOES NOT EXIST\")\n",
    "\n",
    "# Load the averaged state into a new model\n",
    "averaged_model = copy.deepcopy(model1)\n",
    "averaged_model.model.load_state_dict(averaged_state_dict)\n",
    "\n",
    "# Save the averaged model\n",
    "averaged_model.ckpt = {\n",
    "    'model': averaged_model.model\n",
    "}\n",
    "averaged_model.save('./v8m_averaged_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d149ed2-9090-44cb-88c4-736310f1cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_model = YOLO('v8m_averaged_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6c01f6-efc8-4214-b318-f7a988f6d52d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/jupyter/brainhack-til-2025/til-25-main/cv/src/test_images/trucks.jpg: 960x1920 7 trucks, 1 bus, 139.1ms\n",
      "Speed: 98.8ms preprocess, 139.1ms inference, 2077.7ms postprocess per image at shape (1, 3, 960, 1920)\n",
      "Results saved to \u001b[1mtest_images/output\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_path = \"test_images/trucks.jpg\"\n",
    "results1 = averaged_model.predict(input_path, imgsz=(1088,1920), conf=0.1, agnostic_nms=False, iou=0.6, device=0, project=\"test_images\", name=\"output\", save=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2221b1a4-5252-4581-81a5-075e81dd6f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.131 üöÄ Python-3.12.10 torch-2.7.0+cu126 CUDA:0 (Tesla T4, 14918MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'v8m_averaged_model.pt' with input shape (1, 3, 1088, 1920) BCHW and output shape(s) (1, 22, 42840) (49.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.18.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 6.9s, saved as 'v8m_averaged_model.onnx' (99.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.10.0.31...\n",
      "[06/10/2025-09:07:09] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 1783, GPU 950 (MiB)\n",
      "[06/10/2025-09:07:35] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +541, GPU +2, now: CPU 2123, GPU 952 (MiB)\n",
      "[06/10/2025-09:07:35] [TRT] [I] ----------------------------------------------------------------\n",
      "[06/10/2025-09:07:35] [TRT] [I] Input filename:   v8m_averaged_model.onnx\n",
      "[06/10/2025-09:07:35] [TRT] [I] ONNX IR version:  0.0.9\n",
      "[06/10/2025-09:07:35] [TRT] [I] Opset version:    19\n",
      "[06/10/2025-09:07:35] [TRT] [I] Producer name:    pytorch\n",
      "[06/10/2025-09:07:35] [TRT] [I] Producer version: 2.7.0\n",
      "[06/10/2025-09:07:35] [TRT] [I] Domain:           \n",
      "[06/10/2025-09:07:35] [TRT] [I] Model version:    0\n",
      "[06/10/2025-09:07:35] [TRT] [I] Doc string:       \n",
      "[06/10/2025-09:07:35] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 1088, 1920) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 22, 42840) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as v8m_averaged_model.engine\n",
      "[06/10/2025-09:07:35] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[06/10/2025-09:07:37] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[06/10/2025-09:07:37] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/10/2025-09:13:33] [TRT] [I] Detected 1 inputs and 3 output network tensors.\n",
      "[06/10/2025-09:13:36] [TRT] [I] Total Host Persistent Memory: 417712 bytes\n",
      "[06/10/2025-09:13:36] [TRT] [I] Total Device Persistent Memory: 8385024 bytes\n",
      "[06/10/2025-09:13:36] [TRT] [I] Max Scratch Memory: 0 bytes\n",
      "[06/10/2025-09:13:36] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 134 steps to complete.\n",
      "[06/10/2025-09:13:36] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 12.5362ms to assign 9 blocks to 134 nodes requiring 156672512 bytes.\n",
      "[06/10/2025-09:13:36] [TRT] [I] Total Activation Memory: 156672000 bytes\n",
      "[06/10/2025-09:13:36] [TRT] [I] Total Weights Memory: 52254752 bytes\n",
      "[06/10/2025-09:13:37] [TRT] [I] Engine generation completed in 359.475 seconds.\n",
      "[06/10/2025-09:13:37] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 3 MiB, GPU 765 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 401.4s, saved as 'v8m_averaged_model.engine' (52.2 MB)\n",
      "\n",
      "Export complete (401.9s)\n",
      "Results saved to \u001b[1m/home/jupyter/brainhack-til-2025/til-25-main/cv/src\u001b[0m\n",
      "Predict:         yolo predict task=detect model=v8m_averaged_model.engine imgsz=1088,1920 half \n",
      "Validate:        yolo val task=detect model=v8m_averaged_model.engine imgsz=1088,1920 data=None half WARNING ‚ö†Ô∏è non-PyTorch val requires square images, 'imgsz=[1088, 1920]' will not work. Use export 'imgsz=1920' if val is required.\n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'v8m_averaged_model.engine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_model.export(format=\"engine\", imgsz=(1088, 1920), device=0, half=True, nms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ddfb34-fe4b-47ef-8415-7785646169dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "model1 = YOLO('singlemodel/v8l_f7_fixed.pt')\n",
    "model2 = YOLO('singlemodel/v8l_ft_5e.pt')\n",
    "# Get state dictionaries\n",
    "state_dict1 = model1.model.state_dict()\n",
    "state_dict2 = model2.model.state_dict()\n",
    "\n",
    "# Initialize an empty dictionary for the averaged state\n",
    "averaged_state_dict = {}\n",
    "\n",
    "# Average the weights\n",
    "for key in state_dict1.keys():\n",
    "    if key in state_dict2:\n",
    "        averaged_state_dict[key] = (state_dict1[key] + state_dict2[key]) / 2\n",
    "        # print(averaged_state_dict[key])\n",
    "    else:\n",
    "        print(\"ERROR: KEY DOES NOT EXIST\")\n",
    "\n",
    "# Load the averaged state into a new model\n",
    "averaged_model = copy.deepcopy(model1)\n",
    "averaged_model.model.load_state_dict(averaged_state_dict)\n",
    "\n",
    "# Save the averaged model\n",
    "averaged_model.ckpt = {\n",
    "    'model': averaged_model.model\n",
    "}\n",
    "averaged_model.save('./v8l_averaged_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb79110-af52-4ea4-8e2e-8dd3ad8ea566",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "averaged_model = YOLO('v8l_averaged_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f2f6b4-9cfa-4f96-bad5-a561f48a120d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/jupyter/brainhack-til-2025/til-25-main/cv/src/test_images/homemade_test_img2.jpg: 1088x1920 2 commercial aircrafts, 2 drones, 2 fighter jets, 1 fighter plane, 1 helicopter, 2 light aircrafts, 1 missile, 1 tank, 1 van, 2 cargo ships, 2 warships, 294.2ms\n",
      "Speed: 15.8ms preprocess, 294.2ms inference, 2.1ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "Results saved to \u001b[1mtest_images/output\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_path = \"test_images/homemade_test_img2.jpg\"\n",
    "results1 = averaged_model.predict(input_path, imgsz=(1088,1920), conf=0.1, agnostic_nms=False, iou=0.6, device=0, project=\"test_images\", name=\"output\", save=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf68d174-56b8-4724-9449-4849667fb2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.131 üöÄ Python-3.12.10 torch-2.7.0+cu126 CUDA:0 (Tesla T4, 14918MiB)\n",
      "Model summary (fused): 112 layers, 43,620,486 parameters, 0 gradients, 164.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'v8l_averaged_model.pt' with input shape (1, 3, 1088, 1920) BCHW and output shape(s) (1, 22, 42840) (83.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.18.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 7.0s, saved as 'v8l_averaged_model.onnx' (83.7 MB)\n",
      "\n",
      "Export complete (8.9s)\n",
      "Results saved to \u001b[1m/home/jupyter/brainhack-til-2025/til-25-main/cv/src\u001b[0m\n",
      "Predict:         yolo predict task=detect model=v8l_averaged_model.onnx imgsz=1088,1920 half \n",
      "Validate:        yolo val task=detect model=v8l_averaged_model.onnx imgsz=1088,1920 data=None half WARNING ‚ö†Ô∏è non-PyTorch val requires square images, 'imgsz=[1088, 1920]' will not work. Use export 'imgsz=1920' if val is required.\n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'v8l_averaged_model.onnx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_model.export(format=\"onnx\", imgsz=(1088, 1920), device=0, half=True, nms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8ee21-2438-4dab-af86-114f8e9cce8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cv-venv",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "cv-venv",
   "language": "python",
   "name": "cv-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
